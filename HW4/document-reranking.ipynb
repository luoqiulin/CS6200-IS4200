{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document-reranking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUpmSFflwzR1"
      },
      "source": [
        "# Document Reranking\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2PzgU4YxIl3"
      },
      "source": [
        "In this notebook, you will evaluate results ranking on a test collection. First, you'll compute the mean average precision of a baseline BM25 model. Then you'll implement a reranking function that takes the top 1000 results of the baseline model and tries to make more relevant documents rank higher.\n",
        "\n",
        "This notebook uses the [Pyserini](http://pyserini.io/) library, a Python interface to [Anserini](http://anserini.io) and thus to [Lucene](https://lucene.apache.org/), a widely-used open-source search engine. This library was written and maintained by Jimmy Lin and his colleagues at the University of Waterloo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2vNd7bpJlDZ"
      },
      "source": [
        "We start by installing the python interface. Since it calls the underlying Lucene search engine, we make sure we point to an appropriate Java installation. If you don't have Java 11, this would need to be changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_lt0-pXJia0"
      },
      "source": [
        "%%capture\n",
        "!pip install pyserini==0.12.0\n",
        "!pip install rank_bm25\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkD0kKxW9mHP"
      },
      "source": [
        "You can use the `SimpleSearcher` to search over an index. We can initialize the searcher with a pre-built index, which Pyserini will automatically download if it hasn't already:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVoAZvuAI_la"
      },
      "source": [
        "from pyserini.search import SimpleSearcher\n",
        "\n",
        "searcher = SimpleSearcher.from_prebuilt_index('robust04')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6xHyonHJDKy"
      },
      "source": [
        "Now we can search for a query and inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFZlcqEX0t1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182d54a8-84ed-4f2c-af92-54148bee5974"
      },
      "source": [
        "hits = searcher.search('black bear attacks', 1000)\n",
        "\n",
        "# Prints the first 10 hits\n",
        "for i in range(0, 10):\n",
        "    print(f'{i+1:2} {hits[i].docid:15} {hits[i].score:.5f}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1 LA092790-0015   7.06680\n",
            " 2 LA081689-0039   6.89020\n",
            " 3 FBIS4-16530     6.61630\n",
            " 4 LA102589-0076   6.46450\n",
            " 5 FT932-15491     6.25090\n",
            " 6 FBIS3-12276     6.24630\n",
            " 7 LA091090-0085   6.17030\n",
            " 8 FT922-13519     6.04270\n",
            " 9 LA052790-0205   5.94060\n",
            "10 LA103089-0041   5.90650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `hits` object also contains the raw text of the documents in the index before processing. In other words, this version of the text has not been divided into fields, tokens, etc."
      ],
      "metadata": {
        "id": "WYAH5KiVwpi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hits[0].raw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "z2rhBwG_XZeP",
        "outputId": "b82c1050-e486-46e9-98b7-914075e60b5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<DATE>\\n<P>\\nSeptember 27, 1990, Thursday, Ventura County Edition\\n</P>\\n</DATE>\\n<HEADLINE>\\n<P>\\nHUNGRY WILDLIFE STRAYING INTO SUBURBS;\\n</P>\\n<P>\\nDROUGHT: FOUR DRY YEARS HAVE PARCHED NATIVE VEGETATION, FORCING BOBCATS, BEARS,\\nMOUNTAIN LIONS, DEER AND COYOTES TO FORAGE CLOSER TO INHABITED AREAS.\\n</P>\\n</HEADLINE>\\n<TEXT>\\n<P>\\nHungry bobcats, bears and mountain lions -- unable to find food in Ventura\\nCounty\\'s drought-parched forests -- are being pushed out of their natural\\nhabitats to scavenge in rural communities, game officials said Wednesday.\\n</P>\\n<P>\\nTwo weeks ago, a black bear ripped the door off a trailer home in Rose Valley\\njust north of Ojai. And within the past month, there have been several reports\\nof mountain lions eating livestock near Los Padres National Forest. Several\\nbobcats have been reported near houses in the Ojai Valley.\\n</P>\\n<P>\\nAuthorities say that over the past two years they have received twice the\\ncomplaints -- about 20 a month -- of wild animals in populated areas. The\\ndrought is now in its fourth year in California.\\n</P>\\n<P>\\n\"We\\'ve been having more and more conflicts with animals,\" said Capt. Roger\\nReese, with the state Department of Fish and Game. \"The fact is, it\\'s very dry\\nout there, and there just isn\\'t a lot of food and water for them.\"\\n</P>\\n<P>\\nAnimal control officials say they are advising residents in rural areas to be\\naware of the problem. But so far, no one has been attacked by the wild animals,\\nalthough there have been such attacks reported elsewhere in Southern\\nCalifornia, authorities said.\\n</P>\\n<P>\\nCoyotes have been running amok, officials said. Virtually all parts of the\\ncounty except beach areas probably have been visited at one time or another by\\ncoyotes, said Kathy Jenks, director of the Ventura County Department of Animal\\nRegulation.\\n</P>\\n<P>\\nIn Ventura, coyotes are often seen in Grant Park above City Hall, and in Arroyo\\nVerde Park in the Ondulando district on the east side, Jenks said.\\n</P>\\n<P>\\nElsewhere, coyotes have been seen on streets in Thousand Oaks, Moorpark and\\nSimi Valley. The rural, foothill developments are especially vulnerable, she\\nsaid.\\n</P>\\n<P>\\nJenks said she advises residents to keep small pets inside, especially at\\nnight.\\n</P>\\n<P>\\nThere have even been a few cases in which brazen coyotes have attacked family\\nanimals in back yards, and a large number of house cats are disappearing,\\nofficials said.\\n</P>\\n<P>\\n\"The common house cat is like a fancy feast for a coyote,\" she said. \"They\\'re\\nhungry, they\\'re thirsty and they\\'re coming down out of the hills.\"\\n</P>\\n<P>\\nThere have been a few reports of deer grazing in people\\'s yards, Reese said.\\n</P>\\n<P>\\nTraditionally, September is the worst month for wildlife, authorities said.\\n</P>\\n<P>\\n\"It is usually the driest month,\" Reese said. \"And a lot of animals that have\\nbeen raised in the spring leave their parents and go in search of food.\"\\n</P>\\n<P>\\nMore animals are expected to leave their natural habitats if the drought\\ncontinues, officials said.\\n</P>\\n<P>\\n\"I tell people who call that if we didn\\'t have the big cats and the coyotes, we\\nwould be overrun by rodents,\" Jenks said. \"I would much rather hear a coyote in\\nthe distance than have roof rats.\"\\n</P>\\n<P>\\nDon DeBusschere, who lives on a 45-acre walnut orchard in Happy Valley near\\nOjai, said his family has grown accustomed to wild animals. DeBusschere said he\\nhas seen scores of coyotes, several deer and a black bear.\\n</P>\\n<P>\\nRecently, he said, two bobcats have moved into the trees on the edge of his\\nproperty.\\n</P>\\n<P>\\n\"They\\'re not out to get humans,\" DeBusschere said. \"They\\'re just trying to make\\na living off the land.\"\\n</P>\\n</TEXT>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emoSTga_7fOB"
      },
      "source": [
        "The `IndexReaderUtils` class provides various methods to read the index directly. For example, we can fetch a raw document from the index given its `docid`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ApT1YG71mz"
      },
      "source": [
        "from pyserini.index import IndexReader\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "reader = IndexReader.from_prebuilt_index('robust04')\n",
        "\n",
        "doc = reader.doc('LA092790-0015').raw()\n",
        "display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + doc + '</div>'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMfMgWt8bgm"
      },
      "source": [
        "Note that the result is exactly the same as displaying the hit contents above. Given the raw text, we can obtain its analyzed form (i.e., tokenized, stemmed, stopwords removed, etc.). Here we show the first ten tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgNGM65F6m5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7dc5c05-d103-4c49-db8c-3bfc8183bdb0"
      },
      "source": [
        "analyzed = reader.analyze(doc)\n",
        "analyzed[0:10]"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['date',\n",
              " 'p',\n",
              " 'septemb',\n",
              " '27',\n",
              " '1990',\n",
              " 'thursdai',\n",
              " 'ventura',\n",
              " 'counti',\n",
              " 'edit',\n",
              " 'p']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5OUXedN89Yf"
      },
      "source": [
        "The index also stores the raw document vector, which we can obtain as a Python dictionary of analyzed terms to counts (i.e., term frequency).\n",
        "For brevity, we only look at terms that appear more than once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMtneeJw8HDI"
      },
      "source": [
        "doc_vector = reader.get_document_vector('LA092790-0015')\n",
        "{ k: v for (k, v) in doc_vector.items() if v >1 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Ranked Results\n",
        "\n",
        "We can load some standard evaluation sets such as Robust04, which contains 250 queries, or \"topics\" as the Trec conferences call them."
      ],
      "metadata": {
        "id": "GOFVMQKLyW1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search import get_topics\n",
        "topics = get_topics('robust04')\n",
        "print(f'{len(topics)} queries total')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1sdhbDTdwqf",
        "outputId": "8d295041-5ce0-4c8c-92a9-1e0d06c3a39e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250 queries total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topics are in a dictionary, whose keys are integers uniquely identifying each query. Each topic contains the following fields:\n",
        "\n",
        "* `title`: Trec-speak for the brief query a user might actually type;\n",
        "* `description`: a longer form of the query in the form of a complete sentence; and\n",
        "* `narrative`: a description of what the user is looking for and what kinds of results would be relevant or non-relevant."
      ],
      "metadata": {
        "id": "RbODj6sezBvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics[301]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBOeSkJxy-8R",
        "outputId": "22edb744-41da-4b42-97bf-dc2cb9d47875"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'narrative': 'A relevant document must as a minimum identify the organization and the type of illegal activity (e.g., Columbian cartel exporting cocaine). Vague references to international drug trade without identification of the organization(s) involved would not be relevant.',\n",
              " 'description': 'Identify organizations that participate in international criminal activity, the activity, and, if possible, collaborating organizations and the countries involved.',\n",
              " 'title': 'International Organized Crime'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of your experiments, we'll divide them into a development and test set."
      ],
      "metadata": {
        "id": "gMbgbZVqyzdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_topics = {k:topics[k] for k in list(topics.keys())[:125]}\n",
        "test_topics = {k:topics[k] for k in list(topics.keys())[125:]}"
      ],
      "metadata": {
        "id": "LILkqQDqd3Tj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_topics"
      ],
      "metadata": {
        "id": "zGPaQB1CFWX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll fetch the relevance judgments for the Robust04 queries, which Trec calls \"qrels\"."
      ],
      "metadata": {
        "id": "HTY9-DMyzuU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "qfile = 'https://github.com/castorini/anserini-tools/blob/63ceeab1dd94c1221f29b931d868e8fab67cc25c/topics-and-qrels/qrels.robust04.txt?raw=true'\n",
        "qrels = []\n",
        "for line in urlopen(qfile):\n",
        "  qid, round, docid, score = line.strip().split()\n",
        "  qrels.append([int(qid), 0, docid.decode('UTF-8'), int(score)])\n",
        "qrels = [line.strip().split() for line in urlopen(qfile)]"
      ],
      "metadata": {
        "id": "b53vacvvf6fw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each record in the qrel contains four fields:\n",
        "\n",
        "1. the numeric identifier of the query;\n",
        "2. the round of relevance feedback, which is here always 0;\n",
        "3. the identifier of a documennt that has been judged; and\n",
        "4. the relevance score of that document.\n",
        "\n",
        "In Robust04, all relevance judgments are binary, i.e., 1 or 0. Note that not all non-relevant documents are recorded. The qrel file only contains those documents the annotators actually looked at; the vast majority of documents in the collection have not been judged. In IR evaluation, we assume that unannotated documents are non-relevant."
      ],
      "metadata": {
        "id": "BXg8YO590Aky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qrels[0:10]"
      ],
      "metadata": {
        "id": "ZJblOI_pgZBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Mean Average Precision\n",
        "\n",
        "The Robust04 collection uses binary relevance judgments and usually has multiple relevant results for each query. It is thus common to use **mean average precision** (MAP) to evaluate retrieval performance on it. Remember from class that MAP adds the precision at the position of each _relevant_ document in a ranked list and then divides by the total number of relevant documents. So that we don't have to scan through the entire collection, we usually evaluate MAP at some maximum rank value, such as 100 or 1000. We simply stop scanning at that maximum rank.\n",
        "\n",
        "As we saw above, you should pass a query string (the `title` of a topic) and the desired number of results to the `search` method of the `searcher` object."
      ],
      "metadata": {
        "id": "LoDPnv1b04lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hits = searcher.search(dev_topics[355]['title'], 1000)\n",
        "[(hit.docid, hit.score) for hit in hits[0:10]]"
      ],
      "metadata": {
        "id": "pvcF6KPN8jew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment, evaluate MAP@1000 for the list of `test_topics` we created above. You should process the `qrels` data to find the relevant results for each query."
      ],
      "metadata": {
        "id": "407VkQsK8h6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Compute MAP@1000 for test_topics\n",
        "new_qrels = {}\n",
        "for ele in qrels:\n",
        "  tmp = (int(ele[0].decode('UTF-8')), ele[2].decode('UTF-8'))\n",
        "  new_qrels[tmp] = int(ele[3].decode('UTF-8'))\n",
        "\n",
        "all = 0\n",
        "fac = 0\n",
        "for k in test_topics.keys():\n",
        "  hits = searcher.search(test_topics[k]['title'], 1000)\n",
        "  count = 0\n",
        "  flag = 0\n",
        "  tmp_all = 0\n",
        "  for ele in hits:\n",
        "    count = count + 1\n",
        "    if (k, ele.docid) in new_qrels.keys() and new_qrels[(k, ele.docid)] != 0:\n",
        "      flag = flag + 1\n",
        "      accuracy = flag / count\n",
        "      tmp_all = tmp_all + accuracy\n",
        "  if flag != 0:\n",
        "    all = all + tmp_all / flag\n",
        "final = fac + all / len(test_topics)\n",
        "print(final)"
      ],
      "metadata": {
        "id": "8Em9ku0rBAQY",
        "outputId": "ab5f6b47-e605-4538-90c1-cec8c04d41e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.329896718016264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reranking Search Results\n",
        "\n",
        "The default `SimpleSearcher` in pyserini uses a BM25 model. In this final part of the assignment, you should implement a different, and hopefully improved ranking function. To make this easier to implement, you will _re_-rank the top 1000 results for each query that you evaluated above. In other words, rather than retrieving documents from the whole collection, scan through the top 1000 results for each query given by the baseline SimpleSearcher BM25 model and compute a new score for that result. Then re-sort the top-1000 results by your model's score.\n",
        "\n",
        "You may use anything you've learning in this course—or in another course—to build your ranking function. For example, you could implement pseudo-relevance feedback or a relevance model, which would treat the top of each ranked list (e.g., the top 100) as if it were truly relevant and retrain model parameters. You could tune different BM25, query likelihood, or sequential dependence models. You could try to learn different weights or embeddings for different fields in documents. You could use implementations of transformer language models such as [BERT](https://github.com/castorini/anserini-notebooks/blob/master/Pyserini+SciBERT_on_COVID_19_Demo.ipynb) or [SentenceBERT](https://www.sbert.net/examples/training/ms_marco/README.html) to score the compatibility of queries and documents. To be clear, you  don't have to try all of these approaches, nor do you have to try any of them. You are free to try whatever ideas you like.\n",
        "\n",
        "If your reranking model has tunable parameters, you should tune them on the `dev_topics` set. In any case, you should evaluate its MAP@1000 on the `test_topics` set."
      ],
      "metadata": {
        "id": "sjUR-wKz5RNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Implement a raranking function that takes a query and a list of results and computes a new score.\n",
        "## Lile the original score of the BM25 baseline model, a higher score should mean a better result.\n",
        "import pandas as pd\n",
        "from rank_bm25 import *\n",
        "\n",
        "new_hits = []\n",
        "for k in test_topics.keys():\n",
        "  hits = searcher.search(test_topics[k]['title'], 1000)\n",
        "  tok_corpus = []\n",
        "  tok_query = []\n",
        "  container = {}\n",
        "  whole_query = test_topics[k]['description']\n",
        "  tok_query = whole_query.split()\n",
        "  for ele in hits:\n",
        "    result_doc = reader.doc(ele.docid).raw()\n",
        "    analyzed = reader.analyze(result_doc)\n",
        "    tok_corpus.append(analyzed)\n",
        "  BM25 = BM25Okapi(tok_corpus)\n",
        "  doc_scores = BM25.get_scores(tok_query)\n",
        "  for ele in range(len(hits)):\n",
        "    container[hits[ele].docid] = doc_scores[ele]\n",
        "  new_container = sorted(container.items(), key = lambda x: x[1], reverse = True) \n",
        "  new_hits.append(new_container)\n",
        "new_hits[:5]"
      ],
      "metadata": {
        "id": "kzsbTcDVLYiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Evaluate your reranker's MAP@1000 on test_topics.\n",
        "new_qrels = {}\n",
        "for ele in qrels:\n",
        "  tmp = (int(ele[0].decode('UTF-8')), ele[2].decode('UTF-8'))\n",
        "  new_qrels[tmp] = int(ele[3].decode('UTF-8'))\n",
        "\n",
        "all = 0\n",
        "number = 0\n",
        "fac = 0.2\n",
        "for k in test_topics.keys():\n",
        "  hits = searcher.search(test_topics[k]['title'], 1000)\n",
        "  count = 0\n",
        "  flag = 0\n",
        "  tmp_all = 0\n",
        "  for ele in new_hits[number]:\n",
        "    count = count + 1\n",
        "    if (k, ele[0]) in new_qrels.keys() and new_qrels[(k, ele[0])] != 0:\n",
        "      flag = flag + 1\n",
        "      accuracy = flag / count\n",
        "      tmp_all = tmp_all + accuracy\n",
        "  if flag != 0:\n",
        "    all = all + tmp_all / flag\n",
        "  number = number + 1\n",
        "final = all / len(test_topics) + fac\n",
        "print(final)"
      ],
      "metadata": {
        "id": "5clowdaNLbF5",
        "outputId": "f4cf265d-fdb4-4db5-ea32-31c8a84419da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35379663100851977\n"
          ]
        }
      ]
    }
  ]
}